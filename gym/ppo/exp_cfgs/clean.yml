world:
  seed: 42
  map: race3 # either single map or dir to be sampled from in domain randomization
  num_agents: 1
  num_envs:
    count: 8 # set -1 to use number of cpu cores
    type: dummy # subproc or dummy
  num_obstacles: 0 # spawns obstacles in range [0, num_obstacles]
  num_beams: 540 # number of lidar beams
  timestep: 0.01
  observation_config:
    type: frenet_marl # need the nested dict for the observation config
  reset_config:
    type: rl_random_static
  control_input: ['speed', 'steering_angle']
  normalize_input: True
  normalize_observations: False
  normalize_rewards: False
  render_mode: human # set null to disable rendering: only for single env eval / debugging

reward_params:
  vel_action_change_penalty: 0  # -0.5
  steer_action_change_penalty: -0.05  # -1.0
  stagnation_penalty: -0.1
  stagnation_cutoff: 0.02  # delta s as a fraction of total track length
  velocity_reward_scale: 0.0
  heading_penalty: -1.0
  progress_weight: 100
  crash_curriculum: 100000  # int(1e5)
  delta_u_curriculum: 1000000  # int(1e6)
  v_ref_curriculum: 1000000  # int(1e6)
  milestone_reward: 5
  decay_interval: 100000  # 1e5
  max_crash_penalty: 1
  turn_speed_penalty: 0  # -0.1
  initial_crash_penalty: -1.0
  milestone_increment: 0.1
  initial_milestone: 0.1
  overtake_reward: 10.0

car:
  # dr_param_ex:
  # min: x
  # max: y
  mu: 0.8
  C_Sf: 4.718
  C_Sr: 5.4562
  width: 0.31
  length: 0.58
  m: 3.74
  I: 0.04712
  h: 0.074
  v_min: -6.0
  v_max: 6.0
  a_max: 6.34
  s_min: -0.4189
  s_max: 0.4189
  v_switch: 7.319
  sv_min: -3.2
  sv_max: 3.2

ppo_params:
  init_path: null # if resuming a run
  recurrent: False
  device: cpu # actually faster on cpu, for some reason
  learning_rate: 0.0001
  n_steps: 2048
  batch_size: 64
  n_epochs: 8
  gamma: 0.99
  gae_lambda: 0.95
  clip_range: 0.15
  clip_range_vf: null
  normalize_advantage: True
  ent_coef: 0.0
  vf_coef: 0.5
  max_grad_norm: 0.5
  use_sde: False
  sde_sample_freq: -1

train_params:
  total_timesteps: 2_500_000
  save_interval: 500_000 # set large to only save last policy
  log_interval: 1
  reset_num_timesteps: False
  progress_bar: False

log:
  log_tensorboard: True
  project_name: lup_clean # set null to disable wandb
  run_name: test_ego_rewards